{"cells":[{"cell_type":"markdown","metadata":{"id":"oI7CM6qx1Dbr"},"source":["#ResNext + LSTM 모델을 활용"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","\n","import glob\n","import torch\n","import torchvision\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import random"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23170,"status":"ok","timestamp":1714997796605,"user":{"displayName":"강승민","userId":"05010146225716091523"},"user_tz":-540},"id":"qi9Qa0RY079D","outputId":"9723ca4b-6104-42a6-ced5-dbef917d9869"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35844,"status":"ok","timestamp":1714997991987,"user":{"displayName":"강승민","userId":"05010146225716091523"},"user_tz":-540},"id":"j8zzqtmd1cgI","outputId":"4bd93ffa-6b3f-4ef2-e62e-8063c7f4ee76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Collecting face-recognition-models>=0.3.0 (from face_recognition)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.25.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=26bb28769174d57fd3347a340996d9eb382cd6e9387990efde1cb2cdb40372f8\n","  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face_recognition\n","Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"]}],"source":["!pip install face_recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Dv2sDD_4rQ4"},"outputs":[],"source":["import face_recognition\n","\n","# Function to check if an image file is corrupted or not -> 필요없어졌는 지 check\n","def is_corrupted_image(image_path):\n","    try:\n","        _ = cv2.imread(image_path)\n","        return False\n","    except Exception as e:\n","        print(f\"Corrupted image: {image_path}\")\n","        return True\n","\n","# Function to validate a list of image files\n","def validate_images(image_paths, train_transforms):\n","    validated_images = []\n","    for image_path in image_paths:\n","        if not is_corrupted_image(image_path):\n","            try:\n","                image = cv2.imread(image_path)\n","                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB, openCv가 적용하는 image 특성은 BGR이다.\n","                pil_image = Image.fromarray(image) #OpenCv가 읽는 image를 array형태 numerical한 형태 객체인 파이썬 라이브러리를 이용한 PIL 객체로 변환 \n","                pil_image = train_transforms(pil_image) #앞서 만들어둔 image transformation을 적용.\n","                validated_images.append(pil_image)\n","            except Exception as e:\n","                print(f\"Error processing image: {image_path}\")\n","                continue\n","    return validated_images\n","\n","# Constants and configurations\n","#im_size = 112\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","# Define transformations\n","train_transforms = transforms.Compose([\n","    #transforms.Resize((im_size, im_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std) #표준화 이용. 이미지 정규화 하는 부분에서는 Normalization (0-1) 사이의 값, 혹은 Min-Max (-1,1) 사이의 값들을 이룬다.\n","])\n","\n","# Paths to image directories, #list형태 사용\n","train_image_dirs = [ \n","    '/content/drive/MyDrive/datasets/train/real/',\n","    '/content/drive/MyDrive/datasets/train/fake/'\n","]\n","\n","test_image_dirs = [\n","    '/content/drive/MyDrive/datasets/test/real/',\n","    '/content/drive/MyDrive/datasets/test/fake/'\n","]\n","\n","# Get list of all train image files\n","train_image_files = []  #image file들을 저장.(훈련 데이터만 저장을 한다)\n","for dir_path in train_image_dirs:\n","    train_image_files += glob.glob(os.path.join(dir_path, '*.jpg'))  # Assuming images are JPEG format\n","\n","random.shuffle(train_image_files)\n","random.shuffle(train_image_files)\n","\n","#Get list of all test image files\n","test_image_files = []  #image file들을 저장.(테스트 데이터만 저장을 한다)\n","for dir_path in test_image_dirs:\n","    test_image_files += glob.glob(os.path.join(dir_path, '*.jpg'))  # Assuming images are JPEG format\n","\n","random.shuffle(test_image_files)\n","random.shuffle(test_image_files)\n","\n","print(\"Total number of images:\", len(train_image_files)) #train_image_files 2x10000\n","print(\"Total number of images:\", len(test_image_files)) #test_image_files은 2x10000 \n","\n","# Validate images\n","validated_images = validate_images(train_image_files, train_transforms)\n","\n","print(\"Total number of validated images:\", len(validated_images))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpVSdhyO2Ae_"},"outputs":[],"source":["# load the video name and labels from csv\n","class video_dataset(Dataset):\n","    def __init__(self,video_names,labels,sequence_length = 60,transform = None):\n","        self.video_names = video_names\n","        self.labels = labels\n","        self.transform = transform\n","        self.count = sequence_length\n","    def __len__(self):\n","        return len(self.video_names)\n","    def __getitem__(self,idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        a = int(100/self.count)\n","        first_frame = np.random.randint(0,a)\n","        temp_video = video_path.split('/')[-1]\n","        #print(temp_video)\n","        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n","        if(label == 'FAKE'):\n","          label = 0\n","        if(label == 'REAL'):\n","          label = 1\n","        for i,frame in enumerate(self.frame_extract(video_path)):\n","          frames.append(self.transform(frame))\n","          if(len(frames) == self.count):\n","            break\n","        frames = torch.stack(frames)\n","        frames = frames[:self.count]\n","        #print(\"length:\" , len(frames), \"label\",label)\n","        return frames,label\n","    def frame_extract(self,path):\n","      vidObj = cv2.VideoCapture(path) \n","      success = 1\n","      while success:\n","          success, image = vidObj.read()\n","          if success:\n","              yield image\n","#plot the image\n","def im_plot(tensor):\n","    image = tensor.cpu().numpy().transpose(1,2,0)\n","    b,g,r = cv2.split(image)\n","    image = cv2.merge((r,g,b))\n","    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n","    image = image*255.0\n","    plt.imshow(image.astype(int))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#count the number of fake and real videos\n","def number_of_real_and_fake_videos(data_list):\n","  header_list = [\"original_path\",\"id\",\"label\",\"label_str\",\"path\"]\n","  lab = pd.read_csv('/content/drive/MyDrive/datasets/labels/train.csv',names=header_list)\n","  real = 0\n","  fake = 0\n","  #csv파일을 읽었을 때 real이 몇 개인지, fake가 몇 개인지\n","  print(lab[lab['label']].unique()) #1,0\n","  print(lab[lab['label_str']].unique()) #real, fake\n","\n","  #만약에 lab['label']이 1이면 real +=1, 0이면 fake +=0\n","  for label in lab['label']:\n","        if label == 1:\n","            real += 1\n","        else:\n","            fake += 1\n","            \n","  return real,fake"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load the labels and video in data loader\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","header_list = [\"original_path\",\"id\",\"label\",\"label_str\",\"path\"] #우리의 csv파일에 맞게 label list형태를 변환.\n","labels = pd.read_csv('/content/drive/MyDrive/datasets/labels/train.csv',names=header_list)\n","#print(labels)\n","#train image, validation, test image 만드는 코드 (image_file에는 2만장 real, fake가 섞여있는 객체이고 2만장을 8:2로 각각 train : 16000, validation: 4000장)\n","#test image도 4천장 정도 만들어야 한다.\n","\n","train_images = train_image_files[:int(0.8*len(train_image_files))] #처음부터 80%까지의 data를 저장\n","valid_images = train_image_files[int(0.8*len(train_image_files)):] #20% data는 validation image로 저장.\n","test_images = test_image_files[:int(0.2*len(test_image_files))] #20% 즉 4000장 정도 -> 나중에 train_test_split을 통해서 real과 fake의 비율을 맞추는 개선 활동 필요.\n","\n","print(\"train : \" , len(train_images))\n","print(\"test : \" , len(test_images))\n","# train_images,valid_images = train_test_split(image_files,test_size = 0.2)\n","# print(train_videos)\n","\n","print(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_images)[0],\" Fake:\",number_of_real_and_fake_videos(train_images)[1])\n","print(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(valid_images)[0],\" Fake:\",number_of_real_and_fake_videos(valid_images)[1])\n","print(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(test_images)[0],\" Fake:\",number_of_real_and_fake_videos(test_images)[1])\n","\n","\n","im_size = 112\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transforms = transforms.Compose([\n","                                        transforms.ToPILImage(),\n","                                        transforms.Resize((im_size,im_size)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean,std)])\n","\n","test_transforms = transforms.Compose([\n","                                        transforms.ToPILImage(),\n","                                        transforms.Resize((im_size,im_size)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean,std)])\n","train_data = video_dataset(train_videos,labels,sequence_length = 10,transform = train_transforms)\n","#print(train_data)\n","val_data = video_dataset(valid_videos,labels,sequence_length = 10,transform = train_transforms)\n","train_loader = DataLoader(train_data,batch_size = 4,shuffle = True,num_workers = 4)\n","valid_loader = DataLoader(val_data,batch_size = 4,shuffle = True,num_workers = 4)\n","image,label = train_data[0]\n","im_plot(image[0,:,:,:])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOiESqaeEcbcRCjI65OyftV","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
